production: false

image: 
  init:
    repository: busybox
    tag: latest
    pullPolicy: IfNotPresent
  curator:
    repository: bobrik/curator
    tag: latest
    pullPolicy: IfNotPresent

common:
  
  # Defines the service type for all outward-facing (non-discovery) services.
  # For minikube use NodePort otherwise use LoadBalancer
  serviceType: NodePort

  env:
      CLUSTER_NAME: "myesdb"

    # The PVC storage class that backs the persistent volume claims. On AWS
    # "gp2" would be appropriate.
    class: "standard"

# Client/ingest nodes can execute pre-processing pipelines, composed of
# one or more ingest processors. Depending on the type of operations performed
# by the ingest processors and the required resources, it may make sense to
# have dedicated ingest nodes, that will only perform this specific task. 
client:
  pluginsInstall: "" # e.g. "repository-gcs"

  # The amount of RAM allocated to the JVM heap. This should be set to the
  # same value as client.resources.requests.memory, or you may see
  # OutOfMemoryErrors on startup.
  heapSize: "256m"
  replicas: 1
  antiAffinity: "soft"

  resources:
    requests:
      memory: 256Mi

  env:
    NODE_DATA: "false"
    NODE_MASTER: "false"
    NODE_INGEST: "true"
    HTTP_ENABLE: "true"

# Data nodes hold the shards that contain the documents you have indexed. Data
# nodes handle data related operations like CRUD, search, and aggregations. 
# These operations are I/O-, memory-, and CPU-intensive. It is important to 
# monitor these resources and to add more data nodes if they are overloaded.
#
# The main benefit of having dedicated data nodes is the separation of the 
# master and data roles.
data:
  #resources: {}
  # This count will depend on your data and computation needs.
  replicas: 1
  antiAffinity: "soft"

  # The amount of RAM allocated to the JVM heap. This should be set to the
  # same value as data.resources.requests.memory, or you may see
  # OutOfMemoryErrors on startup.
  heapMemory: 256m

  resources:
    requests:
      memory: 256Mi

  storage: 200Gi
  pluginsInstall: "" # e.g. "repository-gcs"

  env:
    NODE_DATA: "true"
    NODE_MASTER: "false"
    NODE_INGEST: "false"
    HTTP_ENABLE: "false"

# The master node is responsible for lightweight cluster-wide actions such as
# creating or deleting an index, tracking which nodes are part of the 
# cluster, and deciding which shards to allocate to which nodes. It is 
# important for cluster health to have a stable master node.
master:
  # Master replica count should be (#clients / 2) + 1, and generally at least 3.
  replicas: 1
  antiAffinity: "soft"
  pluginsInstall: "" # e.g. "repository-gcs"

  # The amount of RAM allocated to the JVM heap. This should be set to the
  # same value as master.resources.requests.memory, or you may see
  # OutOfMemoryErrors on startup.
  heapMemory: 256m

  resources:
    requests:
      memory: 256Mi

  env:
    NODE_DATA: "false"
    NODE_MASTER: "true"
    NODE_INGEST: "false"
    HTTP_ENABLE: "false"
    
    # The default value for this environment variable is 2, meaning a cluster
    # will need a minimum of 2 master nodes to operate. If you have 3 masters
    # and one dies, the cluster still works.
    NUMBER_OF_MASTERS: "1"

curator:
  enabled: false
  schedule: "0 1 * * *"

  # Allows modification of the default age-based filter. If you require more
  # sophisticated filtering, modify the action file specified in
  # templates/es-curator-config.yaml.
  age:
    timestring: "%Y.%m.%d"
    unit: "days"
    unit_count: 3

service:
  httpPort: 9200
  transportPort: 9300

kibana:
  enabled: true
  replicas: 1
  image:
    repository: docker.elastic.co/kibana/kibana-oss
    tag: 6.2.4
    pullPolicy: Always
  httpPort: 80
  resources:
    limits:
      cpu: 1000m
    requests:
      cpu: 100m
  env:
    # XPACK_GRAPH_ENABLED: "false"
    # XPACK_ML_ENABLED: "false"
    # XPACK_REPORTING_ENABLED: "false"
    # XPACK_SECURITY_ENABLED: "false"
  ingress:
    enabled: false
    # Used to create an Ingress record.
    hosts:
      # - kibana.local
    annotations:
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    tls:
      # Secrets must be manually created in the namespace.
      # - secretName: kibana-tls
      #   hosts:
      #     - kibana.local

backup:
  googleApplicationCreds: {}
  #storageClass: "ssd"
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #  cpu: 100m
  #  memory: 128Mi
  # requests:
  #  cpu: 100m
  #  memory: 128Mi
